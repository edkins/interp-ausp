{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1618d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giles/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Loading model: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2 into EasyTransformer!\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# First we grab the model and the unembedding weight matrix\n",
    "import torch\n",
    "from easy_transformer import EasyTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = EasyTransformer.from_pretrained('gpt2').to(device)\n",
    "\n",
    "unembed = model.unembed.W_U.data\n",
    "unembed_bias = model.unembed.b_U.data\n",
    "d_model = model.cfg.d_model\n",
    "d_vocab = model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe72d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# Convenience function for decoding token\n",
    "decode = model.tokenizer.decode\n",
    "\n",
    "def decode_pad(t):\n",
    "    string = decode(t)\n",
    "    return f'{string:10}'\n",
    "\n",
    "# Turn the embedding vector into a set of token probabilities\n",
    "def embed_vector_to_probs(v):\n",
    "    if v.shape != (d_model,):\n",
    "        raise Exception(f\"Shape should be [{d_model}], got {v.shape}\")\n",
    "    logits = torch.matmul(v.to(device), unembed)\n",
    "    return torch.nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "# A function for understanding a vector in embedding space\n",
    "def print_embed_vector(v):\n",
    "    if v.shape != (d_model,):\n",
    "        raise Exception(f\"Shape should be [{d_model}], got {v.shape}\")\n",
    "    logits = torch.matmul(v.to(device), unembed)\n",
    "    probs = torch.nn.functional.softmax(logits, dim=0)\n",
    "    values = [(v.item(),i) for i,v in enumerate(logits)]\n",
    "    values.sort(reverse=True)\n",
    "    for i in range(10):\n",
    "        j = values[i][1]\n",
    "        print(f'{decode_pad(j)} {values[i][0]} {probs[j]}')\n",
    "    print('  ...')\n",
    "    for i in range(d_vocab-5,d_vocab):\n",
    "        j = values[i][1]\n",
    "        print(f'{decode_pad(j)} {values[i][0]} {probs[j]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1cfa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', ' men', '4', ' getting', ' came', ' due', ' simply', ' strong', ' sh', ' common', ' near', 'u', ' Sh', ' further', ' action', ' sp', ' original', ' makes', ' keep', '+', ' including', ' true', ' read', ' years', 'w', 'V', ' recent', ' system', ' social', ' often', ' 16', ' instead', ' ex', 'j', '--', ' John', '%', ' known', 'K', ' above', ' future', ' pro', ' third', 'x', ' business', ' doing', ' women', ' story', ' police', ' possible', 'at', ' war', ' control', ' fl', ' won', ' line', ' started', 'as', ' face', ' news', ' @', ' That', 're', ' along', ' play', ' country', ' political', ' order', '),', ' recently', ' 50', ' add', ' car', ' sc', ' Black', 'z', ' looking', ' rest', ' away', 'J', ' De', ' rather', ' complete', 'le', ' military', ' behind', ' late', ' Not', 'ar', ' actual', ' means', ' try', ' potential', ' six', ' bad', ' What', ' normal', ' history', ' Q', ' personal']\n"
     ]
    }
   ],
   "source": [
    "# Choose some tokens (\"common\" according to the bias vector, but not too common because those are mostly boring stop words)\n",
    "values = [(v.item(), i) for i,v in enumerate(unembed_bias)]\n",
    "values.sort(reverse=True)\n",
    "chosen_tokens = [i for v,i in values[500:600]]\n",
    "print(list(decode(t) for t in chosen_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9edeb212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5          2.070706367492676 0.00015710326260887086\n",
      "6          1.6307480335235596 0.00010118443606188521\n",
      "4          1.6111334562301636 9.921908349497244e-05\n",
      "7          1.5499006509780884 9.332589252153412e-05\n",
      "3          1.4939497709274292 8.824761607684195e-05\n",
      "8          1.4060001373291016 8.081778651103377e-05\n",
      "2          1.3836817741394043 7.903404912212864e-05\n",
      "9          1.3676588535308838 7.777778228046373e-05\n",
      "1          1.237122893333435 6.825972377555445e-05\n",
      "0          1.1361068487167358 6.170122651383281e-05\n",
      "  ...\n",
      "\\\\\\\\\\\\\\\\   -0.3893471360206604 1.3421392395684961e-05\n",
      "nel        -0.412890762090683 1.3109095561958384e-05\n",
      "userc      -0.44671475887298584 1.2673105629801285e-05\n",
      "tain       -0.44970354437828064 1.2635287930606864e-05\n",
      "龍          -0.4547482132911682 1.2571706974995323e-05\n",
      "\n",
      " men\n",
      " men       2.5003812313079834 0.00024101001326926053\n",
      "Men        1.6229230165481567 0.00010022125206887722\n",
      " Men       1.6196537017822266 9.989413229050115e-05\n",
      " man       1.4513418674468994 8.441956015303731e-05\n",
      "men        1.3155549764633179 7.370069215539843e-05\n",
      " women     1.307706594467163 7.312453089980409e-05\n",
      " boys      1.245052456855774 6.868354830658063e-05\n",
      " males     1.2450312376022339 6.86820931150578e-05\n",
      " guys      1.2132786512374878 6.653551827184856e-05\n",
      " MEN       1.2049822807312012 6.598579784622416e-05\n",
      "  ...\n",
      " Rowling   -0.421108603477478 1.2979208804608788e-05\n",
      " Cosponsors -0.4297131896018982 1.2868004887423012e-05\n",
      "],[        -0.43078309297561646 1.2854246961069293e-05\n",
      "zzle       -0.4545218348503113 1.255269398825476e-05\n",
      " sqor      -0.5675336122512817 1.1211314813408535e-05\n",
      "\n",
      "4\n",
      "4          1.9141441583633423 0.00013442918134387583\n",
      "3          1.5782294273376465 9.60743855102919e-05\n",
      "5          1.480984091758728 8.717150194570422e-05\n",
      "2          1.4573873281478882 8.51386139402166e-05\n",
      "6          1.3815058469772339 7.891719724284485e-05\n",
      "8          1.2447510957717896 6.883033347548917e-05\n",
      "7          1.2348638772964478 6.815313827246428e-05\n",
      "1          1.2293477058410645 6.777823000447825e-05\n",
      "9          1.0956964492797852 5.929884355282411e-05\n",
      "0          1.003761887550354 5.4090323828859255e-05\n",
      "  ...\n",
      "tain       -0.35378551483154297 1.3916956959292293e-05\n",
      "WARD       -0.35507169365882874 1.3899067198508419e-05\n",
      " hemisphere -0.35998815298080444 1.3830900570610538e-05\n",
      "\\\\\\\\\\\\\\\\   -0.38152506947517395 1.3536207916331477e-05\n",
      "龍          -0.46164676547050476 1.24939770103083e-05\n",
      "\n",
      " getting\n",
      " getting   2.1833078861236572 0.000175368957570754\n",
      "getting    1.550599455833435 9.314743510913104e-05\n",
      " Getting   1.3352516889572144 7.510115392506123e-05\n",
      " get       1.2989513874053955 7.242384162964299e-05\n",
      "Getting    1.2283819913864136 6.748910527676344e-05\n",
      " gotten    1.221269130706787 6.701076199533418e-05\n",
      " gets      1.16684091091156 6.346097507048398e-05\n",
      " becoming  1.156760811805725 6.282448885031044e-05\n",
      "get        1.1041109561920166 5.960236012469977e-05\n",
      " gaining   1.0923821926116943 5.8907378843287006e-05\n",
      "  ...\n",
      "asar       -0.45121437311172485 1.2583292118506506e-05\n",
      " unfocusedRange -0.45290401577949524 1.2562048141262494e-05\n",
      "rous       -0.4627622067928314 1.2438817975635175e-05\n",
      "soever     -0.5906482934951782 1.0945581379928626e-05\n",
      "kas        -0.6208733320236206 1.0619702152325772e-05\n",
      "\n",
      " came\n",
      " came      2.548200845718384 0.0002524473238736391\n",
      " comes     1.7165573835372925 0.00010989871225319803\n",
      " Came      1.43057119846344 8.256393630290404e-05\n",
      " arose     1.303564429283142 7.271634240169078e-05\n",
      " come      1.2861541509628296 7.146129064494744e-05\n",
      " went      1.2780768871307373 7.088640995789319e-05\n",
      " arrived   1.2725566625595093 7.049617124721408e-05\n",
      " sprang    1.1909593343734741 6.497230788227171e-05\n",
      " took      1.1518501043319702 6.248033605515957e-05\n",
      " became    1.1360043287277222 6.149808905320242e-05\n",
      "  ...\n",
      "oret       -0.4963400959968567 1.2021079783153255e-05\n",
      "hess       -0.5002411007881165 1.1974276276305318e-05\n",
      "ebin       -0.5045369863510132 1.1922947123821359e-05\n",
      "cknow      -0.5081121325492859 1.1880395504704211e-05\n",
      "Adds       -0.517705500125885 1.17669678729726e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Attempt to calculate the \"auspicious vectors\" using the Moore-Penrose pseudoinverse\n",
    "ausp1 = torch.nn.functional.normalize(torch.linalg.pinv(unembed.to('cpu')), dim=1)\n",
    "for t in chosen_tokens[:5]:\n",
    "    print(decode(t))\n",
    "    print_embed_vector(ausp1[t,:])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a79a42",
   "metadata": {},
   "source": [
    "Remarks: we can see that related words are selected together. However, from the probability (softmax, right hand column) we see that the pseudo-inverse is not a good way of isolating a single token value. For that, we'll need to optimize for something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295c128e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We run out of memory if we try to calculate the entire thing at once\n",
    "# So just focus on our chosen tokens for now\n",
    "\n",
    "class SelectorModel(torch.nn.Module):\n",
    "  def __init__(self, ts):\n",
    "    super().__init__()\n",
    "    self.vecs = torch.nn.parameter.Parameter(torch.rand(len(ts), d_model).data)\n",
    "    targ = torch.zeros(len(ts), d_vocab)\n",
    "    for i,t in enumerate(ts):\n",
    "        targ[i,t] = 1\n",
    "    self.target = torch.nn.parameter.Parameter(targ, requires_grad=False)\n",
    "\n",
    "  def get_product(self):\n",
    "    global unembed\n",
    "    return torch.matmul(self.vecs, unembed)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #norm_badness = torch.linalg.matrix_norm(self.vecs)\n",
    "    p = self.get_product()\n",
    "    sm = p.softmax(dim=1)\n",
    "    targ_badness = torch.linalg.matrix_norm(sm - self.target)\n",
    "    #print(norm_badness, targ_badness)\n",
    "    return targ_badness\n",
    "\n",
    "# Compute selector vectors\n",
    "#torch.set_grad_enabled(True)\n",
    "#sel_model = SelectorModel(chosen_tokens).to(device)\n",
    "#optimizer = torch.optim.Adam(sel_model.parameters())\n",
    "#pseudo_data = torch.zeros((1,))\n",
    "#print(\"Created selector model\")\n",
    "#for i in range(1000):\n",
    "#    loss = sel_model(pseudo_data)\n",
    "#    optimizer.zero_grad()\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "#    if i % 100 == 0:\n",
    "#      print(i, 'Loss:', loss.item())\n",
    "#\n",
    "#chosen_ausp = sel_model.vecs.detach()\n",
    "#torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01db5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize it\n",
    "#chosen_ausp = torch.nn.functional.normalize(chosen_ausp, dim=1)\n",
    "#\n",
    "## Print the new auspicious vectors\n",
    "#for i,t in enumerate(chosen_tokens[:5]):\n",
    "#    print(decode(t))\n",
    "#    print_embed_vector(chosen_ausp[i,:])\n",
    "#    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ef075",
   "metadata": {},
   "source": [
    "Next, we want to see if these auspicious vectors can be added to get the result we expect. The expected result is that (a + b) gives a probability vector of 50% a, 50% b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1fd885d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num = 10\n",
    "colors = []\n",
    "for i in range(num):\n",
    "    colors.append([])\n",
    "    for j in range(num):\n",
    "        #v1 = 30 * chosen_ausp[i,:]\n",
    "        #v2 = 30 * chosen_ausp[j,:]\n",
    "        v1 = 4 * ausp1[chosen_tokens[i],:]\n",
    "        v2 = 4 * ausp1[chosen_tokens[j],:]\n",
    "        combo = v1 + v2\n",
    "        probs = embed_vector_to_probs(combo)\n",
    "        r = probs[chosen_tokens[i]].item()\n",
    "        g = probs[chosen_tokens[j]].item()\n",
    "        b = 0\n",
    "        colors[-1].append((r,g,b))\n",
    "        \n",
    "fig, ax = plt.subplots()\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "ax.table(cellColours=colors, rowLabels=[decode(t) for t in chosen_tokens[:num]])\n",
    "ax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
