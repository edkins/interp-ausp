{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b70e9d",
   "metadata": {},
   "source": [
    "The final few layers of a language transformer model are the unembedding. The job of these layers is to turn a vector (in the \"model space\") into a probability distribution over tokens.\n",
    "\n",
    "In the case of gpt2-small, the model space is 768-dimensional and the token vocabulary size is 50257.\n",
    "\n",
    "This consists of:\n",
    "- a learned unembedding _matrix_\n",
    "- a learned _bias vector_, which outputs _logits_\n",
    "- a fixed _softmax layer_, which outputs _probabilities_\n",
    "\n",
    "Let's go in reverse and look at the softmax layer first. We'll use a toy example for illustration with a very small vocabulary, but the concepts generalize to a larger vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b5fbc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size = 7\n",
      "Logits = tensor([ 1.4271, -1.8701, -1.1962, -2.0440, -0.4560, -1.4295, -0.7175])\n",
      "Most likely = 0 cat\n",
      "Probs = tensor([0.6815, 0.0252, 0.0495, 0.0212, 0.1037, 0.0392, 0.0798])\n",
      "     cat        0.6815164685249329\n",
      "     dog        0.025206468999385834\n",
      "     pig        0.049450863152742386\n",
      "     squirrel   0.021183840930461884\n",
      "     the        0.10366861522197723\n",
      "     of         0.03916327282786369\n",
      "     and        0.07981050759553909\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Our example vocabulary\n",
    "vocab = ['cat','dog','pig','squirrel','the','of','and']\n",
    "\n",
    "# The number of words in the vocabulary (or the \"dimension\" of the logit and probability spaces)\n",
    "d_V = len(vocab)\n",
    "print(f\"Vocab size = {d_V}\")\n",
    "\n",
    "# Generate a random vector of logits (the input to softmax). The generator just makes sure this returns the same answer each time\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(12345)\n",
    "logits = torch.normal(mean=0.0, std=1.0, size=(d_V,), generator=generator)\n",
    "\n",
    "print('Logits =', logits)\n",
    "\n",
    "# See which word has the greatest logit. This will be the \"most likely\" in the probability distribution\n",
    "# We print both the index and the corresponding string token\n",
    "most_likely = torch.argmax(logits).item()\n",
    "print(f'Most likely =', most_likely, vocab[most_likely])\n",
    "\n",
    "# Now put it through softmax to obtain the probabilities\n",
    "probs = torch.nn.functional.softmax(logits, dim=0)\n",
    "\n",
    "# Enumerate the probability vector, showing corresponding tokens\n",
    "def print_example_probs(probs):\n",
    "    print('Probs =', probs)\n",
    "    for i in range(d_V):\n",
    "        print('    ', f'{vocab[i]:10}', probs[i].item())\n",
    "print_example_probs(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ecd53",
   "metadata": {},
   "source": [
    "Softmax has some interesting properties. In particular:\n",
    "- Adding a uniform value to each logit leaves the output of softmax unchanged. Softmax only cares about how much things are bigger and smaller than each other, not their actual values\n",
    "- Scaling the vector up (multiplying by a real number >1) will increase the certainty, i.e. make the most likely value more likely\n",
    "- Scaling the vector down (multiplying by a real number <1) will have the opposite effect, making the distribution flatter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f45a71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "Probs = tensor([0.6815, 0.0252, 0.0495, 0.0212, 0.1037, 0.0392, 0.0798])\n",
      "\n",
      "Adding 17 elementwise (this comes out the same):\n",
      "Probs = tensor([0.6815, 0.0252, 0.0495, 0.0212, 0.1037, 0.0392, 0.0798])\n",
      "\n",
      "Multiplying by 2 (makes it more certain):\n",
      "Probs = tensor([9.5442e-01, 1.3056e-03, 5.0250e-03, 9.2214e-04, 2.2084e-02, 3.1517e-03,\n",
      "        1.3089e-02])\n",
      "\n",
      "Multiplying by 0.5 (makes it less certain):\n",
      "Probs = tensor([0.3832, 0.0737, 0.1032, 0.0676, 0.1494, 0.0918, 0.1311])\n"
     ]
    }
   ],
   "source": [
    "print('Original:')\n",
    "print(f'Probs =', torch.nn.functional.softmax(logits, dim=0))\n",
    "print()\n",
    "print('Adding 17 elementwise (this comes out the same):')\n",
    "print(f'Probs =', torch.nn.functional.softmax(logits + 17, dim=0))\n",
    "print()\n",
    "print('Multiplying by 2 (makes it more certain):')\n",
    "print(f'Probs =', torch.nn.functional.softmax(logits * 2, dim=0))\n",
    "print()\n",
    "print('Multiplying by 0.5 (makes it less certain):')\n",
    "print(f'Probs =', torch.nn.functional.softmax(logits * 0.5, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1339d165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that if the softmax layer is given a vector of zeros, it will output equal probability for every token\n",
      "This is ok in our toy example, but in the real dictionary a lot of the tokens are very rare/complete garbage.\n",
      "We don't want to output equal probability for those.\n",
      "Probs = tensor([0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429])\n"
     ]
    }
   ],
   "source": [
    "print(\"Note that if the softmax layer is given a vector of zeros, it will output equal probability for every token\")\n",
    "print(\"This is ok in our toy example, but in the real dictionary a lot of the tokens are very rare/complete garbage.\")\n",
    "print(\"We don't want to output equal probability for those.\")\n",
    "zeros = torch.zeros((d_V,))\n",
    "print(f'Probs =', torch.nn.functional.softmax(zeros))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9ccab",
   "metadata": {},
   "source": [
    "Now that we understand softmax, let's look at the previous layers.\n",
    "\n",
    "Because some tokens are a lot more probable than others (in general), it helps to have a \"bias layer\" that adds the approximate token commonness to each token, before the softmax is applied. This can be \"overridden\" in some sense - given a sufficiently large vector identifying one particular token, that token will still win even if the bias layer is against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efa02ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (without bias vector):\n",
      "Probs = tensor([0.6815, 0.0252, 0.0495, 0.0212, 0.1037, 0.0392, 0.0798])\n",
      "     cat        0.6815164685249329\n",
      "     dog        0.025206468999385834\n",
      "     pig        0.049450863152742386\n",
      "     squirrel   0.021183840930461884\n",
      "     the        0.10366861522197723\n",
      "     of         0.03916327282786369\n",
      "     and        0.07981050759553909\n",
      "\n",
      "With bias vector added:\n",
      "Probs = tensor([0.5061, 0.0187, 0.0223, 0.0035, 0.2093, 0.0791, 0.1611])\n",
      "     cat        0.506083607673645\n",
      "     dog        0.01871793530881405\n",
      "     pig        0.022272685542702675\n",
      "     squirrel   0.003510014619678259\n",
      "     the        0.2092607021331787\n",
      "     of         0.07905318588018417\n",
      "     and        0.1611018180847168\n",
      "\n",
      "If the original vector is large enough, it can overcome the bias and the squirrel wins\n",
      "Probs = tensor([0.0039, 0.0039, 0.0024, 0.9579, 0.0106, 0.0106, 0.0106])\n",
      "     cat        0.0039146095514297485\n",
      "     dog        0.0039146095514297485\n",
      "     pig        0.0023743310011923313\n",
      "     squirrel   0.9578734040260315\n",
      "     the        0.010641012340784073\n",
      "     of         0.010641012340784073\n",
      "     and        0.010641012340784073\n"
     ]
    }
   ],
   "source": [
    "bias = torch.tensor([1.0, 1.0, 0.5, -0.5, 2.0, 2.0, 2.0])\n",
    "\n",
    "print(\"Original (without bias vector):\")\n",
    "print_example_probs(torch.nn.functional.softmax(logits, dim=0))\n",
    "\n",
    "print()\n",
    "print(\"With bias vector added:\")\n",
    "print_example_probs(torch.nn.functional.softmax(logits + bias, dim=0))\n",
    "\n",
    "print()\n",
    "print(\"If the original vector is large enough, it can overcome the bias and the squirrel wins\")\n",
    "big_squirrel_logits = torch.tensor([0, 0, 0, 7, 0, 0, 0])\n",
    "print_example_probs(torch.nn.functional.softmax(big_squirrel_logits + bias, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f52ed9",
   "metadata": {},
   "source": [
    "It's important not to forget the bias vector is there. But, we're not going to focus on it here.\n",
    "\n",
    "Instead, we'll be looking at the weights matrix. This can be thought of as a linear map from the model's internal vector space to the token logit space.\n",
    "\n",
    "In this artificial example, we'll be pretending the model space is two dimensional. This is unrealistic but will help with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb6d6755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "\n",
    "ts = [0, 0.5, 1, 2, 4, 4.5, 5]\n",
    "\n",
    "# The weights matrix\n",
    "M_to_V = torch.tensor([[math.cos(t), math.sin(t)] for t in ts]).T\n",
    "print(M_to_V.shape)\n",
    "\n",
    "plt.scatter(M_to_V[0,:], M_to_V[1,:], color=(0,0.9,1))\n",
    "for t in range(d_V):\n",
    "    plt.text(M_to_V[0,t], M_to_V[1,t], vocab[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5e5f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
