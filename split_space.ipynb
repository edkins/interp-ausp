{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc24b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Loading model: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2 into EasyTransformer!\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from easy_transformer import EasyTransformer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "model = EasyTransformer.from_pretrained('gpt2').to(device)\n",
    "\n",
    "# Convenience function for decoding token\n",
    "decode = model.tokenizer.decode\n",
    "\n",
    "# Convenience function for encoding token\n",
    "def encode(t):\n",
    "    global model\n",
    "    result = model.tokenizer.encode(t)\n",
    "    if len(result) != 1:\n",
    "        raise Exception(f\"Not a single token: {t}\")\n",
    "    return result[0]\n",
    "\n",
    "unembed = model.unembed.W_U.data\n",
    "embed = model.embed.W_E.data\n",
    "d_M = model.cfg.d_model\n",
    "d_V = model.cfg.d_vocab\n",
    "\n",
    "unembed_norm = torch.nn.functional.normalize(unembed, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c3f0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.m = torch.nn.parameter.Parameter(torch.nn.functional.normalize(torch.normal(0,1,(d_M,d_M)), dim=0).detach())\n",
    "        self.eye = torch.nn.parameter.Parameter(torch.eye(d_M).detach(), requires_grad=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        proj = torch.matmul(self.m.reshape(1, d_M, d_M), x)\n",
    "        n0 = torch.linalg.vector_norm(proj[:,:d_M//2], dim=1)\n",
    "        n1 = torch.linalg.vector_norm(proj[:,d_M//2:], dim=1)\n",
    "        nm = torch.linalg.matrix_norm(torch.matmul(self.m, self.m.T) - self.eye)\n",
    "        return torch.maximum(nm, torch.minimum(n0, n1) / (n0 + n1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de299e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9223c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42142.12109375\n",
      "3169.84814453125\n",
      "3094.2080078125\n",
      "3004.171630859375\n",
      "3129.3564453125\n",
      "2982.315673828125\n",
      "2904.879638671875\n",
      "2815.7685546875\n",
      "2779.310791015625\n",
      "2753.8740234375\n",
      "2720.760498046875\n",
      "2732.699462890625\n",
      "2813.479248046875\n",
      "2730.5810546875\n",
      "2626.838623046875\n",
      "2599.5673828125\n",
      "2652.43212890625\n",
      "2778.969482421875\n",
      "2555.29150390625\n",
      "2542.2626953125\n",
      "2519.49853515625\n",
      "2588.35400390625\n",
      "2668.92138671875\n",
      "2930.870849609375\n",
      "2502.163330078125\n",
      "2477.93408203125\n",
      "2461.53857421875\n",
      "2448.879150390625\n",
      "2441.616455078125\n",
      "2442.08544921875\n",
      "2498.725830078125\n",
      "2470.211669921875\n",
      "2417.249755859375\n",
      "2419.16552734375\n",
      "2407.465576171875\n",
      "2421.244140625\n",
      "2572.24658203125\n",
      "2396.061279296875\n",
      "2402.81103515625\n",
      "2395.29736328125\n",
      "2377.490478515625\n",
      "2400.641845703125\n",
      "2637.973876953125\n",
      "2359.19677734375\n",
      "2431.5263671875\n",
      "2351.19775390625\n",
      "2370.3115234375\n",
      "2518.938232421875\n",
      "2458.9609375\n",
      "2338.905029296875\n",
      "2384.572021484375\n",
      "2333.310791015625\n",
      "2427.00830078125\n",
      "2472.6923828125\n",
      "2317.54833984375\n",
      "2326.238525390625\n",
      "2324.6171875\n",
      "2324.80126953125\n",
      "2327.014404296875\n",
      "2357.1552734375\n",
      "2786.71337890625\n",
      "2332.00537109375\n",
      "2310.4482421875\n",
      "2306.291259765625\n",
      "2363.418701171875\n",
      "2308.556640625\n",
      "2321.985595703125\n",
      "2702.960693359375\n",
      "2345.053466796875\n",
      "2304.734375\n",
      "2302.43115234375\n",
      "2305.728759765625\n",
      "2317.634765625\n",
      "2302.658203125\n",
      "2304.534423828125\n",
      "3098.494384765625\n",
      "2614.255615234375\n",
      "2298.28173828125\n",
      "2301.01220703125\n",
      "2296.900390625\n",
      "2300.93017578125\n",
      "2298.61181640625\n",
      "2318.713134765625\n",
      "2455.734375\n",
      "2342.532958984375\n",
      "2295.262451171875\n",
      "2288.796142578125\n",
      "2344.056396484375\n",
      "2293.51220703125\n",
      "2441.294677734375\n",
      "2383.238525390625\n",
      "2304.82080078125\n",
      "2296.65380859375\n",
      "2284.708984375\n",
      "2291.77294921875\n",
      "2296.78076171875\n",
      "2597.0146484375\n",
      "2423.623291015625\n",
      "2451.655029296875\n",
      "2339.79296875\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model = OrthModel().to(device)\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "total_loss = torch.zeros(()).to(device)\n",
    "for i in range(10000):\n",
    "    v = unembed[:, random.choices(range(d_V), k=64)]\n",
    "    optim.zero_grad()\n",
    "    loss = model(v)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.detach().sum()\n",
    "    if i % 100 == 99:\n",
    "        print(total_loss.item())\n",
    "        total_loss = torch.zeros(()).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e407f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31485  clauses 2.2015624046325684 4.288162708282471\n",
      "12525  ally 1.978772521018982 3.9806699752807617\n",
      "10277 uz 2.947826385498047 3.9549875259399414\n",
      "49511  Useful 2.2316324710845947 3.57596492767334\n",
      "48772  dehuman 1.7610663175582886 4.324909210205078\n",
      "1024  De 2.0421578884124756 3.0899152755737305\n",
      "49863  UFOs 1.950603723526001 4.148329734802246\n",
      "41723  hesitated 2.081098794937134 3.7363429069519043\n",
      "17442 hett 3.2232377529144287 4.6391496658325195\n",
      "37058 Generally 1.6794567108154297 3.5697903633117676\n",
      "44544  Khalid 2.1657705307006836 4.27364444732666\n",
      "47627 Pitt 2.0935235023498535 4.602341175079346\n",
      "23348 falls 3.1604034900665283 4.276700019836426\n",
      "6433 ework 3.395779848098755 4.734602451324463\n",
      "6171  syn 2.0913803577423096 4.118034362792969\n",
      "29224  Leicester 1.5256083011627197 3.992495059967041\n",
      "24259 Details 2.1429195404052734 3.9507293701171875\n",
      "10499  Stadium 2.3156659603118896 3.9979496002197266\n",
      "14800  fierce 1.6110990047454834 3.693042278289795\n",
      "7694  pref 1.9805717468261719 3.760049343109131\n",
      "5471  opposition 1.9234068393707275 3.69513201713562\n",
      "43991  Lavrov 2.185471773147583 4.344596862792969\n",
      "41087  Cheap 2.25537109375 3.9179372787475586\n",
      "40894  spacious 1.608849048614502 3.719517707824707\n",
      "49160  Cutter 2.822359800338745 4.502195358276367\n",
      "48825 nit 3.0950682163238525 4.435984134674072\n",
      "21239  Cir 2.3632972240448 4.059191703796387\n",
      "972 ement 3.337205410003662 4.208858966827393\n",
      "18608 supp 2.3231208324432373 4.127083778381348\n",
      "50247  (/ 2.8202383518218994 3.5162999629974365\n",
      "39599  plank 2.133500814437866 4.303582668304443\n",
      "47459  gratification 2.531879186630249 4.013409614562988\n",
      "26981  Transform 2.1278719902038574 3.9711732864379883\n",
      "37001  undermines 1.5467045307159424 3.650282382965088\n",
      "47793  Sutherland 2.4385712146759033 4.686341762542725\n",
      "37812  tame 2.1191465854644775 4.098186016082764\n",
      "17852  EL 1.9218286275863647 3.858595371246338\n",
      "45632 ة 2.7064058780670166 4.498807430267334\n",
      "45947  HERO 2.4181814193725586 4.33214807510376\n",
      "45347  avert 2.0746655464172363 3.910780668258667\n",
      "12786 vals 2.883843183517456 4.4994659423828125\n",
      "39125  benefiting 1.933416485786438 3.688906669616699\n",
      "14151  Info 2.343106985092163 3.7657418251037598\n",
      "35948  Influence 2.177874803543091 4.033408164978027\n",
      "7142  regime 1.9244240522384644 3.8614206314086914\n",
      "31598  TI 2.00242280960083 4.0912394523620605\n",
      "39063  Afterwards 2.358238935470581 3.5222973823547363\n",
      "46393  432 1.5305078029632568 3.5512430667877197\n",
      "36586  Reggie 1.8491829633712769 3.9485695362091064\n",
      "38099  Grac 2.311272382736206 4.279338836669922\n",
      "6746  Saud 2.752791404724121 4.6166205406188965\n",
      "9763  backed 2.009418487548828 3.5913949012756348\n",
      "25336  meme 1.5011541843414307 3.7298319339752197\n",
      "21728  BUT 2.5253701210021973 3.001631498336792\n",
      "25082 Happy 1.9475218057632446 3.925724506378174\n",
      "14978 pection 2.985854387283325 4.387356758117676\n",
      "45441  docking 2.0650086402893066 4.319099426269531\n",
      "24737  disciplinary 1.8026281595230103 3.946009635925293\n",
      "16844 Requ 2.243997812271118 4.34055233001709\n",
      "30333  inflic 2.136631727218628 4.073501110076904\n",
      "9966  Albert 1.67461097240448 3.901812791824341\n",
      "17354 Updated 2.5525879859924316 3.9466631412506104\n",
      "37806  whispers 1.8678621053695679 4.031154155731201\n",
      "20662 \"} 3.9866795539855957 3.386199951171875\n",
      "44854  shuts 1.6943951845169067 3.820549488067627\n",
      "29800 Multi 2.270700454711914 3.8391740322113037\n",
      "24183  declares 1.6550495624542236 3.6943130493164062\n",
      "12393 Day 2.548353433609009 3.8175599575042725\n",
      "32545  shrug 2.051813840866089 3.957406997680664\n",
      "27524 Opp 2.0363776683807373 4.003861904144287\n",
      "43444 bda 3.3418209552764893 4.495584487915039\n",
      "29442  hypocrisy 1.8925286531448364 3.938602924346924\n",
      "4089 98 1.7540886402130127 3.4986116886138916\n",
      "44256 toggle 2.933069944381714 4.828775405883789\n",
      "34653 Uh 1.9382189512252808 3.706862211227417\n",
      "2856 cil 3.1923704147338867 4.4234299659729\n",
      "44510 hander 3.2250187397003174 4.360552787780762\n",
      "26098  inline 2.1817386150360107 3.7869014739990234\n",
      "49293 urated 3.260780096054077 4.658401012420654\n",
      "5447  defined 1.9137203693389893 3.65610408782959\n",
      "3922 ilies 3.03488826751709 4.369030952453613\n",
      "22997 ゴン 3.807166337966919 5.9453325271606445\n",
      "34755 eme 3.240832567214966 4.22442102432251\n",
      "14245  firmly 1.8621567487716675 3.590535879135132\n",
      "32456  advisors 1.8340742588043213 4.088248252868652\n",
      "25648 ucing 2.403934955596924 3.672319173812866\n",
      "33987  Almighty 2.447108507156372 4.257593154907227\n",
      "38659 connection 2.0874621868133545 4.053455352783203\n",
      "46286 jon 3.020918607711792 4.239339351654053\n",
      "42577  barb 2.0392894744873047 4.002621650695801\n",
      "49485  joystick 1.784459114074707 3.8938205242156982\n",
      "45108  congratulations 2.5667009353637695 4.222368240356445\n",
      "33211  lubric 1.781604290008545 4.1752166748046875\n",
      "220   2.502455711364746 3.0553836822509766\n",
      "39103 0100 2.6183226108551025 3.9971675872802734\n",
      "19988  advancing 1.8667023181915283 3.742828130722046\n",
      "31699 fuck 2.849188804626465 4.395968914031982\n",
      "8551  recover 1.7995935678482056 4.053022861480713\n",
      "29338  Quin 2.305636167526245 4.345770835876465\n",
      "26325  adhere 1.873050332069397 3.761615514755249\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "mat = model.m\n",
    "for i in random.sample(range(d_V), k=100):\n",
    "    v = unembed[:,i]\n",
    "    w = torch.matmul(mat, v)\n",
    "    n0 = torch.linalg.vector_norm(w[:d_M//2]).item()\n",
    "    n1 = torch.linalg.vector_norm(w[d_M//2:]).item()\n",
    "    print(i, decode(i), n0, n1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
